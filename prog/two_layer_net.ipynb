{
 "cells": [
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Réseau de neurones à deux couches cachées\n",
    "\n",
    "Pour cet exercice, vous devez coder en partie un réseau de neurones pleinenement connecté et tester ses performances sur la base de données CIFAR-10."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "The autoreload extension is already loaded. To reload it, use:\n  %reload_ext autoreload\n"
    }
   ],
   "source": [
    "# A bit of setup\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from ift725.classifiers.two_layer_neural_net import TwoLayerNeuralNet\n",
    "\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (10.0, 8.0) # set default size of plots\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['image.cmap'] = 'gray'\n",
    "\n",
    "# for auto-reloading external modules\n",
    "# see http://stackoverflow.com/questions/1907993/autoreload-of-modules-in-ipython\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "def rel_error(x, y):\n",
    "  \"\"\" returns relative error \"\"\"\n",
    "  return np.max(np.abs(x - y) / (np.maximum(1e-8, np.abs(x) + np.abs(y))))"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Nous utiliseraons la classe `TwoLayerNeuralNet` dans le fichier `ift725/classifiers/two_layer_neural_net.py` pour entraîner et tester le réseau de neurones. Les paramètres du réseau sont stockés dans la variable dictionnaire `self.params` dont les clés sont les noms des paramètres et les valeurs sont des tableaux numpy. Ci-après est un example jouet (modèle et données) qu'on utilisera pour tester le code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a small net and some toy data to check your implementations.\n",
    "# Note that we set the random seed for repeatable experiments.\n",
    "\n",
    "input_size = 4\n",
    "hidden_size = 10\n",
    "num_classes = 3\n",
    "num_inputs = 5\n",
    "\n",
    "def init_toy_model():\n",
    "  np.random.seed(0)\n",
    "  return TwoLayerNeuralNet(input_size, hidden_size, num_classes, std=1e-1)\n",
    "\n",
    "def init_toy_data():\n",
    "  np.random.seed(1)\n",
    "  X = 10 * np.random.randn(num_inputs, input_size)\n",
    "  y = np.array([0, 1, 2, 2, 1])\n",
    "  return X, y\n",
    "\n",
    "net = init_toy_model()\n",
    "X, y = init_toy_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Propagation avant : calcul de la perte (loss)\n",
    "Même fonction, maintenant implanter la perte (entropie croisée) + régularisation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "Your scores:\n[[-0.81233741 -1.27654624 -0.70335995]\n [-0.17129677 -1.18803311 -0.47310444]\n [-0.51590475 -1.01354314 -0.8504215 ]\n [-0.15419291 -0.48629638 -0.52901952]\n [-0.00618733 -0.12435261 -0.15226949]]\n\ncorrect scores:\n[[-0.81233741 -1.27654624 -0.70335995]\n [-0.17129677 -1.18803311 -0.47310444]\n [-0.51590475 -1.01354314 -0.8504215 ]\n [-0.15419291 -0.48629638 -0.52901952]\n [-0.00618733 -0.12435261 -0.15226949]]\n\nDifference between your scores and correct scores:\n3.6802720745909845e-08\n"
    }
   ],
   "source": [
    "scores = net.loss(X)\n",
    "print('Your scores:')\n",
    "print(scores)\n",
    "print()\n",
    "print('correct scores:')\n",
    "correct_scores = np.asarray([\n",
    "  [-0.81233741, -1.27654624, -0.70335995],\n",
    "  [-0.17129677, -1.18803311, -0.47310444],\n",
    "  [-0.51590475, -1.01354314, -0.8504215 ],\n",
    "  [-0.15419291, -0.48629638, -0.52901952],\n",
    "  [-0.00618733, -0.12435261, -0.15226949]])\n",
    "print(correct_scores)\n",
    "print()\n",
    "\n",
    "# The difference should be very small. We get < 1e-7\n",
    "print('Difference between your scores and correct scores:')\n",
    "print(np.sum(np.abs(scores - correct_scores)))"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Propagation avant et calcul des scores de classes\n",
    "\n",
    "Dans le fichier `ift725/classifiers/two_layer_neural_net.py` il y a une méthode nommée `loss`. Cette fonction est très similaire aux fonctions de pertes que vous avez codé aux excercices précédents. \n",
    "\n",
    "Ici vous devez coder la propagation avant du réseau afin de calculer le score de chaque classe.\n",
    "\n",
    "**NOTE:** Utilisez la fonction d'activation **ReLU**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss, _ = net.loss(X, y, reg=0.1)\n",
    "correct_loss = 1.3417187305415879\n",
    "\n",
    "# should be very small, we get < 1e-12\n",
    "print('Difference between your loss and correct loss:')\n",
    "print(np.sum(np.abs(loss - correct_loss)))"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rétro-propagation\n",
    "Coder le reste de la fonction `loss(.)`. Vous devrez ainsi coder le gradient de la loss par rapport aux variables `W1`, `b1`, `W2`, et `b2`. À noter que si votre propagation avant fonctionne, vous pourrez tester vos gradients grâce à l'approximation numérique du gradient\n",
    "\n",
    "\n",
    "# Question 1 :\n",
    "\n",
    "Avant de commencer à coder, donnez l'équation mathématique du gradient de la perte par rapport aux paramètres `W1`, `W2`, `b1` et `b2` et ce, considérant que une mini-batch de N données contenue dans le tableau \n",
    "\n",
    "$$X \\in R^{N\\times d}$$\n",
    "\n",
    "où d est la dimensionnalité des données.  Votre réponse doit être linéarisée et donc ne contenir aucune boucle for.\n",
    "\n",
    "\n",
    "**Votre Réponse:** \n",
    "\n",
    "$$dW_1 = (X'*dReLU(f*W2'))/N + 2*reg*W1 $$\n",
    "\n",
    "$$dW_2 = (a1'*f)/N + 2*reg*W2$$\n",
    "\n",
    "$$db_1 = \\frac{1}{N} \\Sigma  dReLU(f*W2')$$\n",
    "\n",
    "$$db_2 = \\frac{1}{N} \\Sigma  f$$\n",
    "\n",
    "où f est le score softmax, a1 est le résultat du ReLU, reg le facteur de régularisation et dReLU la dérivé de ReLU. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ift725.gradient_check import evaluate_numerical_gradient\n",
    "\n",
    "# Use numeric gradient checking to check your implementation of the backward pass.\n",
    "# If your implementation is correct, the difference between the numeric and\n",
    "# analytic gradients should be less than 1e-8 for each of W1, W2, b1, and b2.\n",
    "\n",
    "loss, grads = net.loss(X, y, reg=0.1)\n",
    "\n",
    "# these should all be less than 1e-8 or so\n",
    "for param_name in grads:\n",
    "  f = lambda W: net.loss(X, y, reg=0.1)[0]\n",
    "  param_grad_num = evaluate_numerical_gradient(f, net.params[param_name], verbose=False)\n",
    "  print('%s max relative error: %e' % (param_name, rel_error(param_grad_num, grads[param_name])))"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Entraînement\n",
    "Pour entraîner le réseau, nous utiliserons SGD tout comme pour les exercices précédents. Voir la fonction `train`  et remplir les section `TODO`. Vous devez également coder la fonction `predict` car SGD prédict périodiquement des résultats afin de mesurer l'évolution de la justesse (accuracy) au fil du temps.\n",
    "\n",
    "La cellule suivante vous permet d'entraîner le réseau sur des données jouets et obtenir d'excellents résultats."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = init_toy_model()\n",
    "stats = net.train(X, y, X, y,\n",
    "            learning_rate=1e-1, reg=1e-5,\n",
    "            num_iters=100, verbose=False)\n",
    "\n",
    "print('Final training loss: ', stats['loss_history'][-1])\n",
    "\n",
    "# plot the loss history\n",
    "plt.plot(stats['loss_history'])\n",
    "plt.xlabel('iteration')\n",
    "plt.ylabel('training loss')\n",
    "plt.title('Training Loss history')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Charger les images \n",
    "Testons maintenant les performances du réseau sur une vraie base de données: **CIFAR-10**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ift725.data_utils import load_CIFAR10\n",
    "\n",
    "def get_CIFAR10_data(num_training=49000, num_validation=1000, num_test=1000):\n",
    "    \"\"\"\n",
    "    Load the CIFAR-10 dataset from disk and perform preprocessing to prepare\n",
    "    it for the two-layer neural net classifier. These are the same steps as\n",
    "    we used for the SVM, but condensed to a single function.  \n",
    "    \"\"\"\n",
    "    # Load the raw CIFAR-10 data\n",
    "    cifar10_dir = 'ift725/datasets/cifar-10-batches-py'\n",
    "    X_train, y_train, X_test, y_test = load_CIFAR10(cifar10_dir)\n",
    "        \n",
    "    # Subsample the data\n",
    "    mask = range(num_training, num_training + num_validation)\n",
    "    X_val = X_train[mask]\n",
    "    y_val = y_train[mask]\n",
    "    mask = range(num_training)\n",
    "    X_train = X_train[mask]\n",
    "    y_train = y_train[mask]\n",
    "    mask = range(num_test)\n",
    "    X_test = X_test[mask]\n",
    "    y_test = y_test[mask]\n",
    "\n",
    "    # Normalize the data: subtract the mean image\n",
    "    mean_image = np.mean(X_train, axis=0)\n",
    "    X_train -= mean_image\n",
    "    X_val -= mean_image\n",
    "    X_test -= mean_image\n",
    "\n",
    "    # Reshape data to rows\n",
    "    X_train = X_train.reshape(num_training, -1)\n",
    "    X_val = X_val.reshape(num_validation, -1)\n",
    "    X_test = X_test.reshape(num_test, -1)\n",
    "\n",
    "    return X_train, y_train, X_val, y_val, X_test, y_test\n",
    "\n",
    "\n",
    "# Invoke the above function to get our data.\n",
    "X_train, y_train, X_val, y_val, X_test, y_test = get_CIFAR10_data()\n",
    "print('Train data shape: ', X_train.shape)\n",
    "print('Train labels shape: ', y_train.shape)\n",
    "print('Validation data shape: ', X_val.shape)\n",
    "print('Validation labels shape: ', y_val.shape)\n",
    "print('Test data shape: ', X_test.shape)\n",
    "print('Test labels shape: ', y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Entraîner le réseau\n",
    "Pour entraîner le réseau, utilisez SGD avec momentum.  De plus, après chaque epoch, réduisez le taux d'apprentissage (learning_rate) en le multiplicant par le taux de décroissance (learning_rate_decay)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = 32 * 32 * 3\n",
    "hidden_size = 50\n",
    "num_classes = 10\n",
    "net = TwoLayerNeuralNet(input_size, hidden_size, num_classes)\n",
    "\n",
    "# Train the network\n",
    "stats = net.train(X_train, y_train, X_val, y_val,\n",
    "            num_iters=3000, batch_size=200,\n",
    "            learning_rate=8e-5, learning_rate_decay=0.98,\n",
    "            reg=0.5, verbose=True)\n",
    "\n",
    "# Predict on the validation set\n",
    "val_acc = (net.predict(X_val) == y_val).mean()\n",
    "print('Validation accuracy: ', val_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualiser l'entraînement\n",
    "Avec les paramètres par défaut, vous devriez obtenir une justesse infériure à 0.4 sur l'ensemble de validation. \n",
    "\n",
    "Pour visualiser les résultats de l'entraînement, il est courant d'afficher les courbes d'apprentissage ainsi que les poids de la première couche."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the loss function and train / validation accuracies\n",
    "plt.subplot(2, 1, 1)\n",
    "plt.plot(stats['loss_history'])\n",
    "plt.title('Loss history')\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('Loss')\n",
    "\n",
    "plt.subplot(2, 1, 2)\n",
    "plt.plot(stats['train_acc_history'], label='train')\n",
    "plt.plot(stats['val_acc_history'], label='val')\n",
    "plt.title('Classification accuracy history')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Clasification accuracy')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ift725.vis_utils import visualize_as_grid\n",
    "\n",
    "# Visualize the weights of the network\n",
    "\n",
    "def show_net_weights(net):\n",
    "  W1 = net.params['W1']\n",
    "  W1 = W1.reshape(32, 32, 3, -1).transpose(3, 0, 1, 2)\n",
    "  plt.imshow(visualize_as_grid(W1, padding=3).astype('uint8'))\n",
    "  plt.gca().axis('off')\n",
    "  plt.show()\n",
    "\n",
    "show_net_weights(net)"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ajustement de hyperparamètres\n",
    "\n",
    "Dans la prochaine cellule, vous devez rédiger du code permettant de choisir les hyper-paramètres (learning_rate et regularization) qui permettent d'atteindre une justesse de validation d'environ 0.5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_net = None # store the best model into this \n",
    "best_acc = -1\n",
    "best_stats = None\n",
    "results = {}\n",
    "\n",
    "#################################################################################\n",
    "# TODO: Ajustez les hyper-paramètres en utilisant les données de validations.   #\n",
    "#  Stockez le meilleur modèle de réseau de neurones dans la variable \"best_net\".#\n",
    "#                                                                               #\n",
    "# Pour vous aider à déboguer votre réseau, il peut être utile d'utiliser une    #\n",
    "# visualisation similaire à celle utilisée juste avant; ces visualisations      #\n",
    "# auront une différence qualitative différente de celle vu plus avant pour le   #\n",
    "# réseau faiblement ajusté.                                                     #\n",
    "#                                                                               #\n",
    "# Ajuster les hyper-paramètres manuellement peut être amusant, mais vous devriez#\n",
    "# trouver utile d'écrire du code qui balai l'ensemble des combinaisons          #\n",
    "# d'hyper-paramètres possibles de façon automatique comme vous avez fait lors   #\n",
    "# de l'exercice précédent.                                                      #\n",
    "#################################################################################\n",
    "\n",
    "\n",
    "    \n",
    "#################################################################################\n",
    "#                              FIN DE VOTRE CODE                                #\n",
    "#################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the cross-validation results\n",
    "import math\n",
    "marker_size = 100\n",
    "x_scatter, x_label = [np.log10(x[0]) for x in results], 'log learning rate'\n",
    "y_scatter, y_label = [np.log10(x[1]) for x in results], 'log regularization'\n",
    "\n",
    "# plot validation accuracy\n",
    "colors = [results[x] for x in results] # default size of markers is 20\n",
    "plt.subplot(2, 1, 2)\n",
    "plt.scatter(x_scatter, y_scatter, marker_size, c=colors)\n",
    "plt.colorbar()\n",
    "plt.xlabel(x_label)\n",
    "plt.ylabel(y_label)\n",
    "plt.title('CIFAR-10 validation accuracy')\n",
    "plt.show()\n",
    "\n",
    "print('Best val acc:', best_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the loss function and train / validation accuracies\n",
    "plt.subplot(2, 1, 1)\n",
    "plt.plot(best_stats['loss_history'])\n",
    "plt.title('Loss history')\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('Loss')\n",
    "\n",
    "plt.subplot(2, 1, 2)\n",
    "plt.plot(best_stats['train_acc_history'], label='train', color='blue')\n",
    "plt.plot(best_stats['val_acc_history'], label='val', color='green')\n",
    "plt.title('Classification accuracy history')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Clasification accuracy')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize the weights of the best network\n",
    "show_net_weights(best_net)"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exécution du modèle sur les données test\n",
    "Une fois terminé, il est temps de mesurer la justesse de notre modèle sur les données test.  Vous devriez obtenir une justesse d'environ 0.45"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_acc = (best_net.predict(X_test) == y_test).mean()\n",
    "print('Test accuracy: ', test_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Question 2\n",
    "(a) Donnez en octets la quantité de mémoire requise pour stocker les paramètres de votre réseau.\n",
    "\n",
    "(b) Donnez en octets la quantité de mémoire requise pour entraîner votre réseau lorsque vous utilisez des images CIFAR et une taille de batch de 10.\n",
    "\n",
    "# réponse : \n",
    "(a) Le réseau est composé de : \n",
    "\n",
    "1. 4 neurones en entrée\n",
    "2. 10 neurones dans la couche cachée\n",
    "3. 3 neurones de sortie\n",
    "\n",
    "Donc 4x10 + 10 + 3x10 + 3 = 83 paramètres. Chaque paramètre est sauvés dans un float64, soit 4 octets de mémoire. La quantité de mémoire requise pour les paramètres est alors: 83x4 = 332 octets.\n",
    "\n",
    "(b) Pour un batch de 10, les tailles de matrices pour garder les images en mémoire seraient:\n",
    "\n",
    "1. 10 x 4 en entrée\n",
    "2. 10 x 10 dans la couche cachée\n",
    "3. 10 x 3 en sortie\n",
    "\n",
    "Chaque valeur étant aussi un float64, on a : (10x4 + 10x10 + 10x3)x4 = 680 octets\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}